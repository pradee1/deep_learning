{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-class classification Keras_tf(MNIST).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradee1/deep_learning/blob/master/Multi_class_classification_Keras_tf(MNIST).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P2R41W7e__8",
        "colab_type": "text"
      },
      "source": [
        "# Classifying Handwritten Digits with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evY_NExafVDW",
        "colab_type": "text"
      },
      "source": [
        "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitCdah3hYBc",
        "colab_type": "text"
      },
      "source": [
        "**Learning Objectives:**\n",
        "  * Train a neural network to classify handwritten digits from the classic [MNIST](http://yann.lecun.com/exdb/mnist/) data set using keras and tensorflow\n",
        "  * Visualize the weights of a neural-network hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Tk9Ta6fSTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZordCY0Chpvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_train_small.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ9prOeuhtg6",
        "colab_type": "code",
        "outputId": "1d667387-98bf-4c7a-d28e-eef55e794d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "mnist_dataframe.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3    4    5    6    ...  778  779  780  781  782  783  784\n",
              "0    6    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "1    5    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "2    7    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "3    9    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "4    5    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOMAdHB0hwSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOKbBUHOjmQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This returns a tensor\n",
        "inputs = Input(shape=(784,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = Dense(128, activation='relu')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "\n",
        "\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "my_model = Model(inputs=inputs, outputs= predictions)\n",
        "my_model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
        "                 metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlJQHJy54hbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
        "                 metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNWWlm1tkHI6",
        "colab_type": "code",
        "outputId": "abad09dd-eb24-4867-f03e-0331c2259c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "my_model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 131,594\n",
            "Trainable params: 130,826\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7XKw-HykaHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array(mnist_dataframe.drop(0,axis=1))\n",
        "y = np.array(mnist_dataframe[[0]])\n",
        "y = to_categorical(y,num_classes=None,dtype='float32')\n",
        "num_epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT9BkcbrlFWN",
        "colab_type": "code",
        "outputId": "e2817fb6-1fd3-4cf6-f1bb-6d078e7da25c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "my_model.fit(x=x, y=y, batch_size=128, epochs=num_epochs,verbose=2, validation_split=0.25)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            " - 4s - loss: 0.9556 - acc: 0.6934 - val_loss: 0.3416 - val_acc: 0.8966\n",
            "Epoch 2/20\n",
            " - 1s - loss: 0.3907 - acc: 0.8800 - val_loss: 0.2635 - val_acc: 0.9202\n",
            "Epoch 3/20\n",
            " - 1s - loss: 0.2995 - acc: 0.9077 - val_loss: 0.1992 - val_acc: 0.9402\n",
            "Epoch 4/20\n",
            " - 1s - loss: 0.2460 - acc: 0.9265 - val_loss: 0.1807 - val_acc: 0.9488\n",
            "Epoch 5/20\n",
            " - 1s - loss: 0.2223 - acc: 0.9327 - val_loss: 0.1682 - val_acc: 0.9496\n",
            "Epoch 6/20\n",
            " - 1s - loss: 0.1902 - acc: 0.9419 - val_loss: 0.1589 - val_acc: 0.9510\n",
            "Epoch 7/20\n",
            " - 1s - loss: 0.1692 - acc: 0.9478 - val_loss: 0.1497 - val_acc: 0.9572\n",
            "Epoch 8/20\n",
            " - 1s - loss: 0.1589 - acc: 0.9512 - val_loss: 0.1477 - val_acc: 0.9572\n",
            "Epoch 9/20\n",
            " - 1s - loss: 0.1363 - acc: 0.9581 - val_loss: 0.1320 - val_acc: 0.9616\n",
            "Epoch 10/20\n",
            " - 1s - loss: 0.1366 - acc: 0.9553 - val_loss: 0.1392 - val_acc: 0.9568\n",
            "Epoch 11/20\n",
            " - 1s - loss: 0.1247 - acc: 0.9611 - val_loss: 0.1236 - val_acc: 0.9624\n",
            "Epoch 12/20\n",
            " - 1s - loss: 0.1161 - acc: 0.9642 - val_loss: 0.1226 - val_acc: 0.9630\n",
            "Epoch 13/20\n",
            " - 1s - loss: 0.1090 - acc: 0.9637 - val_loss: 0.1209 - val_acc: 0.9650\n",
            "Epoch 14/20\n",
            " - 1s - loss: 0.0978 - acc: 0.9687 - val_loss: 0.1256 - val_acc: 0.9636\n",
            "Epoch 15/20\n",
            " - 1s - loss: 0.0973 - acc: 0.9694 - val_loss: 0.1310 - val_acc: 0.9624\n",
            "Epoch 16/20\n",
            " - 1s - loss: 0.0850 - acc: 0.9729 - val_loss: 0.1212 - val_acc: 0.9654\n",
            "Epoch 17/20\n",
            " - 1s - loss: 0.0911 - acc: 0.9701 - val_loss: 0.1137 - val_acc: 0.9674\n",
            "Epoch 18/20\n",
            " - 1s - loss: 0.0861 - acc: 0.9722 - val_loss: 0.1229 - val_acc: 0.9628\n",
            "Epoch 19/20\n",
            " - 1s - loss: 0.0870 - acc: 0.9712 - val_loss: 0.1263 - val_acc: 0.9614\n",
            "Epoch 20/20\n",
            " - 1s - loss: 0.0797 - acc: 0.9735 - val_loss: 0.1162 - val_acc: 0.9670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3732ee8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcR8OviM1WTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "367c60b6-ac9c-4127-a447-b68877cb6c0b"
      },
      "source": [
        " my_model.history.history.keys()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpJm3JUrWBPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss_history = my_model.history.history['val_loss']\n",
        "tra_loss_history = my_model.history.history['loss']\n",
        "val_acc_history = my_model.history.history['val_acc']\n",
        "tra_acc_history = my_model.history.history['acc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVH1xabUxy-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "363dd4a6-cdb4-4e71-8642-3441160781b6"
      },
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.subplot(211)\n",
        "plt.plot(val_loss_history)\n",
        "plt.plot(tra_loss_history)\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(val_acc_history)\n",
        "plt.plot(tra_acc_history)\n",
        "\n",
        "plt.ylabel(\"Acc\")\n",
        "\n",
        "#plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFpCAYAAACbJYGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OorAjaqfxzK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rejo7B3VxzNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhlNNrHYxyGa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKhf6i443MEP",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Custom Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KjgViZ33QiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njLfuBUS3ZNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAIXfy5c3fM5",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLYenWaI3dBE",
        "colab_type": "code",
        "outputId": "0a70b202-6b6b-4cf9-f214-6297c85c3c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset_url = \"https://download.mlcc.google.com/mledu-datasets/mnist_train_small.csv\"\n",
        "dataset_file = tf.keras.utils.get_file(fname=os.path.basename(dataset_url),origin=dataset_url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://download.mlcc.google.com/mledu-datasets/mnist_train_small.csv\n",
            "36528128/36523880 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTnmCyUMNZsS",
        "colab_type": "code",
        "outputId": "f098e3ed-6e27-4408-8a4d-77c0a03151a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset_file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/mnist_train_small.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAkH-YrS4Wny",
        "colab_type": "code",
        "outputId": "6aa83fde-13c4-4db9-e911-d1b036be9200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!head -n5 {dataset_file}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,67,67,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,131,252,252,66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,159,250,232,30,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,222,252,108,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,147,252,183,5,0,0,0,0,0,0,0,20,89,89,73,0,0,0,0,0,0,0,0,0,0,0,0,48,247,252,159,0,0,0,0,0,0,0,79,236,252,252,249,198,16,0,0,0,0,0,0,0,0,0,41,193,252,199,22,0,0,0,0,0,12,135,248,252,252,252,252,252,100,0,0,0,0,0,0,0,0,0,100,252,252,88,0,0,0,0,0,11,171,252,252,235,175,178,252,252,224,0,0,0,0,0,0,0,0,15,209,252,233,12,0,0,0,0,49,177,252,252,89,26,0,2,166,252,252,0,0,0,0,0,0,0,0,96,253,253,59,0,0,0,0,11,177,255,253,92,0,0,0,0,155,253,128,0,0,0,0,0,0,0,0,143,252,252,10,0,0,0,12,171,252,216,110,13,0,0,0,3,180,232,24,0,0,0,0,0,0,0,0,143,252,170,2,0,0,0,135,252,209,19,0,0,0,0,0,12,252,132,0,0,0,0,0,0,0,0,0,249,252,96,0,0,0,21,248,246,34,0,0,0,0,5,61,234,152,3,0,0,0,0,0,0,0,0,0,253,252,44,0,0,0,145,252,104,0,0,0,46,114,184,252,149,34,0,0,0,0,0,0,0,0,0,0,253,252,82,0,0,31,239,252,66,39,89,165,243,252,233,126,5,0,0,0,0,0,0,0,0,0,0,0,249,252,244,126,98,143,252,252,237,240,253,252,243,174,17,0,0,0,0,0,0,0,0,0,0,0,0,0,119,239,252,252,252,252,252,252,252,252,228,179,17,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,66,66,66,66,66,66,66,66,29,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,59,50,0,23,0,0,32,134,180,254,206,8,0,0,0,0,0,0,0,0,0,0,0,0,4,96,216,233,254,248,215,231,215,215,236,254,250,181,27,0,0,0,0,0,0,0,0,0,0,0,0,0,108,254,254,247,175,175,175,176,175,175,205,175,60,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,254,245,85,0,0,0,0,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,152,254,158,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,240,255,38,0,41,50,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,87,254,254,178,215,242,248,215,96,19,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,176,254,254,254,217,175,187,254,254,248,85,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,161,247,214,57,11,0,3,19,177,248,248,129,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,49,0,0,0,0,0,0,0,57,224,254,171,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,213,255,122,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,92,254,196,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,40,254,196,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,145,254,196,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,188,45,0,0,0,0,0,0,0,99,249,254,121,0,0,0,0,0,0,0,0,0,0,0,0,0,0,139,245,45,0,0,0,0,0,0,140,254,254,133,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,242,169,0,0,0,0,4,58,216,248,254,167,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,230,196,79,49,79,79,181,254,254,247,108,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,44,213,254,247,254,254,254,254,192,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,133,156,193,155,140,58,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,97,179,254,223,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,65,185,235,253,254,253,253,199,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,37,55,0,61,224,253,253,253,192,78,226,253,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,100,228,247,159,248,254,234,183,64,5,0,177,253,161,0,0,0,0,0,0,0,0,0,0,0,0,0,76,254,253,253,253,253,193,46,0,0,0,0,214,253,117,0,0,0,0,0,0,0,0,0,0,0,0,0,121,255,254,254,146,60,0,0,0,0,0,14,224,254,57,0,0,0,0,0,0,0,0,0,0,0,0,79,244,254,243,106,3,0,0,0,0,0,0,186,253,216,10,0,0,0,0,0,0,0,0,0,0,0,0,166,253,254,135,0,0,0,0,0,0,0,0,254,253,107,0,0,0,0,0,0,0,0,0,0,0,0,126,251,253,146,3,0,0,0,0,0,0,0,106,254,242,36,0,0,0,0,0,0,0,0,0,0,0,8,205,253,215,23,0,0,0,0,0,0,0,31,239,254,121,0,0,0,0,0,0,0,0,0,0,0,0,178,254,244,83,0,0,0,0,0,0,0,19,201,254,196,15,0,0,0,0,0,0,0,0,0,0,0,28,232,253,124,0,0,0,0,0,0,0,2,129,253,253,15,0,0,0,0,0,0,0,0,0,0,0,0,18,220,174,13,0,0,0,0,0,0,0,88,253,253,154,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,0,0,0,0,0,0,0,0,10,175,253,231,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,134,253,253,138,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83,255,254,152,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,222,254,191,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,137,253,254,135,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,234,253,254,173,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,159,253,193,46,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,114,181,219,255,196,126,122,22,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,74,212,218,254,254,225,217,216,245,133,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,104,187,253,213,129,56,15,0,0,86,237,88,0,0,0,0,0,0,0,0,0,0,0,0,0,0,82,235,254,254,143,0,0,0,0,0,0,10,254,113,0,0,0,0,0,0,0,0,0,0,0,0,0,0,226,254,254,70,4,0,0,0,0,0,0,1,50,100,22,79,111,0,0,0,0,0,0,0,0,0,0,0,226,254,254,71,4,45,93,90,90,19,5,23,207,228,228,254,243,73,0,0,0,0,0,0,0,0,0,0,195,254,254,254,193,232,254,254,254,254,198,254,254,254,254,254,254,131,0,0,0,0,0,0,0,0,0,0,19,176,235,254,254,254,254,254,254,254,254,254,254,254,254,254,254,111,0,0,0,0,0,0,0,0,0,0,0,0,25,125,131,162,225,215,131,201,131,169,209,254,254,254,216,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,254,254,222,30,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,223,254,254,145,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,73,254,254,152,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,200,254,252,66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,101,254,254,143,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,254,254,95,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,157,254,213,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,235,254,85,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,88,254,251,66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,160,254,153,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,91,225,15,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,20,206,210,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,179,248,239,122,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,78,192,237,237,206,237,249,254,239,123,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,48,229,254,254,235,254,254,174,114,26,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,155,254,132,35,23,35,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,232,232,23,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,131,254,140,0,12,63,51,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,254,126,61,172,254,254,254,166,82,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,254,230,254,250,208,160,245,251,254,237,135,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,254,254,241,92,0,0,0,52,133,248,254,225,46,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,255,241,61,0,0,0,0,0,0,22,153,254,242,89,0,0,0,0,0,0,0,0,0,0,0,0,0,26,127,49,0,0,0,0,0,0,0,0,20,153,254,227,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,228,252,57,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,211,254,61,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,211,254,61,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,49,246,241,32,0,0,0,0,0,0,0,0,0,0,0,0,0,33,159,105,0,0,0,0,0,0,0,42,217,254,179,0,0,0,0,0,0,0,0,0,0,0,0,0,26,218,254,71,0,0,0,0,0,18,122,248,254,215,13,0,0,0,0,0,0,0,0,0,0,0,0,0,39,241,254,208,97,91,43,128,185,251,254,252,184,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,93,242,254,254,254,254,254,254,254,202,71,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydqtbd945Ada",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_names = list(range(785))\n",
        "feature_names = column_names[1:]\n",
        "label_name = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-_VvvEPK8ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "training_dataset = tf.data.experimental.make_csv_dataset(dataset_file,batch_size,\n",
        "                                                    column_names = column_names,\n",
        "                                                    label_name = label_name,\n",
        "                                                    shuffle_seed = 42,\n",
        "                                                   num_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmpa9QzLPhxm",
        "colab_type": "code",
        "outputId": "2d5ce32f-cb8f-4594-cdab-fedb0b033d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "features, labels = next(iter(training_dataset))\n",
        "features[1], "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=2410, shape=(64,), dtype=int32, numpy=\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       dtype=int32)>,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ZuEYPpVmEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pack_features_vector(features, labels):\n",
        "  \"\"\"Pack the features into a single array.\"\"\"\n",
        "  features = tf.cast(tf.stack(list(features.values()), axis=1),dtype=tf.float32)\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaS55osWE4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_dataset = training_dataset.map(pack_features_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZB1jBlwWHB7",
        "colab_type": "code",
        "outputId": "19636016-ae35-4952-8a19-828fe24a4ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#\n",
        "features, labels = next(iter(training_dataset))\n",
        "print(features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(64, 784), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siPmmPsKXd4-",
        "colab_type": "text"
      },
      "source": [
        "##Model: Architecture, loss and gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b12fmd4OWM6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This returns a tensor\n",
        "inputs = tf.keras.layers.Input(shape=(784,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = tf.keras.layers.Dense(128, activation='relu',)(inputs)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "predictions = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "#my_model = tf.keras.models.Model(inputs=inputs, outputs= predictions)\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs= predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0phELCoXvFZ",
        "colab_type": "code",
        "outputId": "e1e1b0ae-e6c7-4440-850e-baa293b62b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 130,058\n",
            "Trainable params: 130,058\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9kOqK1gNZxe",
        "colab_type": "code",
        "outputId": "23552311-8eba-48e6-df37-30c2baa6b500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "predictions = model(features)\n",
        "predictions[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=4175, shape=(5, 10), dtype=float32, numpy=\n",
              "array([[1.0000000e+00, 0.0000000e+00, 6.5809958e-37, 0.0000000e+00,\n",
              "        3.0715901e-24, 3.1713464e-29, 2.9846028e-32, 0.0000000e+00,\n",
              "        3.5825859e-30, 0.0000000e+00],\n",
              "       [1.1364060e-11, 0.0000000e+00, 8.4719689e-18, 7.3227194e-23,\n",
              "        1.5803199e-28, 2.0987479e-22, 4.5576949e-19, 0.0000000e+00,\n",
              "        1.0000000e+00, 2.6102458e-33],\n",
              "       [5.5950814e-01, 0.0000000e+00, 5.7661250e-25, 0.0000000e+00,\n",
              "        6.6369250e-24, 5.3820564e-29, 5.1267076e-14, 0.0000000e+00,\n",
              "        4.4049183e-01, 6.8030595e-23],\n",
              "       [9.9999011e-01, 0.0000000e+00, 2.8764610e-10, 8.4874515e-24,\n",
              "        7.3985780e-06, 2.2248410e-20, 7.0400532e-08, 0.0000000e+00,\n",
              "        2.4233834e-06, 4.1305686e-18],\n",
              "       [1.0000000e+00, 0.0000000e+00, 2.5315462e-13, 0.0000000e+00,\n",
              "        0.0000000e+00, 6.9953462e-35, 5.1405756e-15, 0.0000000e+00,\n",
              "        1.0434267e-09, 0.0000000e+00]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "726NrNOiNZv3",
        "colab_type": "code",
        "outputId": "679057cf-b91a-464b-c314-4ef3a228b61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "tf.nn.softmax(predictions[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=4181, shape=(5, 10), dtype=float32, numpy=\n",
              "array([[0.23196931, 0.08533674, 0.08533674, 0.08533674, 0.08533674,\n",
              "        0.08533674, 0.08533674, 0.08533674, 0.08533674, 0.08533674],\n",
              "       [0.08533674, 0.08533674, 0.08533674, 0.08533674, 0.08533674,\n",
              "        0.08533674, 0.08533674, 0.08533674, 0.23196931, 0.08533674],\n",
              "       [0.15480563, 0.08846988, 0.08846988, 0.08846988, 0.08846988,\n",
              "        0.08846988, 0.08846988, 0.08846988, 0.1374354 , 0.08846988],\n",
              "       [0.23196736, 0.08533687, 0.08533687, 0.08533687, 0.0853375 ,\n",
              "        0.08533687, 0.08533688, 0.08533687, 0.08533709, 0.08533687],\n",
              "       [0.23196931, 0.08533674, 0.08533674, 0.08533674, 0.08533674,\n",
              "        0.08533674, 0.08533674, 0.08533674, 0.08533674, 0.08533674]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uo0rFfzNZs0",
        "colab_type": "code",
        "outputId": "e5c9962c-b9a3-4eb1-b2c7-6dea4d8d04d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"Prediction: {}\".format(tf.argmax(predictions, axis=1)))\n",
        "print(\"    Labels: {}\".format(labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [0 8 0 0 0 8 8 8 0 8 8 8 8 8 0 8 0 0 8 8 0 8 0 0 8 0 8 8 0 8 8 0 0 0 8 8 0\n",
            " 6 0 0 6 0 6 8 0 8 0 8 8 8 2 0 0 8 0 6 0 8 0 8 0 8 8 2]\n",
            "    Labels: [5 5 0 5 4 7 2 3 3 5 1 3 5 3 5 5 1 9 2 1 5 8 4 1 1 4 3 3 7 5 4 9 4 7 0 8 1\n",
            " 5 4 7 0 0 0 8 1 1 4 2 4 2 0 7 7 5 5 6 7 3 6 8 2 3 3 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlitFDzoNZqe",
        "colab_type": "code",
        "outputId": "d3a16da3-c145-43c6-8105-fd805ec7d958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "def loss(model, x, y):\n",
        "  y_ = model(x)\n",
        "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
        "\n",
        "\n",
        "l = loss(model, features, labels)\n",
        "print(\"Loss test: {}\".format(l))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Loss test: 2.357140302658081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phAzmtJxNZoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhFMnHp7NZlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "\n",
        "global_step = tf.Variable(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SJDzE3mNZiY",
        "colab_type": "code",
        "outputId": "ad8d2aa8-05fd-40a1-c7ea-9a3b8dfcce9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "loss_value, grads = grad(model, features, labels)\n",
        "\n",
        "print(\"Step: {}, Initial Loss: {}\".format(global_step.numpy(),\n",
        "                                          loss_value.numpy()))\n",
        "\n",
        "optimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)\n",
        "\n",
        "print(\"Step: {},         Loss: {}\".format(global_step.numpy(),\n",
        "                                          loss(model, features, labels).numpy()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0, Initial Loss: 2.357140302658081\n",
            "Step: 1,         Loss: 2.3674001693725586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RW6R7kSLrDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbP4p2BRNZfX",
        "colab_type": "code",
        "outputId": "2b1a2987-7752-45d4-d990-d23b45d6993d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "## Note: Rerunning this cell uses the same model variables\n",
        "\n",
        "from tensorflow import contrib\n",
        "tfe = contrib.eager\n",
        "\n",
        "# keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tfe.metrics.Mean()\n",
        "  epoch_accuracy = tfe.metrics.Accuracy()\n",
        "  starttime=time()\n",
        "  \n",
        "  # Training loop - using batches of 32\n",
        "  for x, y in training_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value, grads = grad(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
        "                              global_step)\n",
        "\n",
        "    # Track progress\n",
        "    epoch_loss_avg(loss_value)  # add current batch loss\n",
        "    # compare predicted label to actual label\n",
        "    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n",
        "  time_taken = time()-starttime\n",
        "  # end epoch\n",
        "  train_loss_results.append(epoch_loss_avg.result())\n",
        "  train_accuracy_results.append(epoch_accuracy.result())\n",
        "  \n",
        "  if epoch % 1 == 0:\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}, time: {}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result(),time_taken ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Epoch 000: Loss: 2.363, Accuracy: 9.810%, time: 13.667140245437622\n",
            "Epoch 001: Loss: 2.363, Accuracy: 9.810%, time: 13.47936463356018\n",
            "Epoch 002: Loss: 2.363, Accuracy: 9.810%, time: 13.571603536605835\n",
            "Epoch 003: Loss: 2.363, Accuracy: 9.810%, time: 13.663497686386108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1ce81c3d0c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Training loop - using batches of 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \"\"\"\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1832\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1834\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1835\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zxeHKlqNZcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "801cg-LWNZ--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmg2sRlSNZ82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHUSGPQgNZ6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw38PGEBNZ4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcVQA2FWNZ2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcFGhOchNZ0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L9zd3u_NZxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eqSxwV5NZva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlQicjsQFFZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}